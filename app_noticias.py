# -*- coding: utf-8 -*-
"""Buscador de Not√≠cias sobre Contrave / Merck.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pRzf98ffqo-D8KmamRMYR4XCiOVxjBCr
"""

# -*- coding: utf-8 -*-
"""Buscador de Not√≠cias sobre Contrave / Merck """

# -----------------------------
# Importa√ß√µes
# -----------------------------
import streamlit as st
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from dateutil import parser
from io import BytesIO
import requests
from bs4 import BeautifulSoup

# -----------------------------
# Configura√ß√µes
# -----------------------------
SENHA_APP = "MAP2025"  # Defina sua senha aqui

TERMS_PADRAO = [
    "Contrave", "Bupropiona", "Naltrexona",
    "Bupropiona + Naltrexona", "Contrave XR",
    "Emagrecimento farmacol√≥gico",
    "Medicamento para obesidade",
    "Tratamento de obesidade",
    "Merck",
    "Eurofarma"
]

RSS_FEEDS = [
    "https://g1.globo.com/rss/g1/economia/",
    "https://g1.globo.com/rss/g1/saude/",
    "https://www.cnnbrasil.com.br/feed/",
    "https://www.uol.com.br/feed/noticias/saude/",
    "https://www1.folha.uol.com.br/rss/saude.xml",
    "https://www.gov.br/anvisa/pt-br/assuntos/noticias?format=rss",
    "https://www.fda.gov/about-fda/press-announcements.atom",
    "https://abc-farma.com.br/feed/",
    "https://www.kairosnews.com.br/feed/",
    "https://www.pharmaceutical-technology.com/feed/",
    "https://www.merck.com/news/rss.xml",
    "https://www.eurofarma.com.br/noticias/feed/",
    "https://www.pfizer.com/news/rss.xml",
    "https://www.novartis.com/news/media-releases/rss.xml",
    "https://www.roche.com/media/newsfeed.xml"
]

SCRAPING_URLS = [
    "https://www.gov.br/anvisa/pt-br/assuntos/medicamentos/cmed/precos"
]

# -----------------------------
# Streamlit App
# -----------------------------
st.title("üîç Buscador de Not√≠cias sobre Contrave / Merck")
st.markdown("Busque not√≠cias recentes em diversos sites sobre medicamentos e empresas farmac√™uticas.")

# -----------------------------
# Autentica√ß√£o por senha
# -----------------------------
senha_input = st.text_input("Digite a senha para acessar o app:", type="password")
if senha_input != SENHA_APP:
    st.warning("Senha incorreta! Digite a senha correta para acessar o app.")
    st.stop()

# -----------------------------
# Termos e dias de busca
# -----------------------------
st.subheader("üìå Termos de busca padr√£o")
st.write(", ".join(TERMS_PADRAO))

novos_termos = st.text_input("Adicionar termos extras separados por v√≠rgula", "")
TERMS = TERMS_PADRAO + [t.strip() for t in novos_termos.split(",") if t.strip() != ""]

DIAS_BUSCA = st.number_input("Buscar not√≠cias dos √∫ltimos X dias", min_value=1, max_value=365, value=180)

# -----------------------------
# Sites consultados
# -----------------------------
st.subheader("üåê Sites que ser√£o consultados")
for site in RSS_FEEDS + SCRAPING_URLS:
    st.write("-", site)

# -----------------------------
# Fun√ß√£o de busca RSS
# -----------------------------
def buscar_rss(feed_url, dias=DIAS_BUSCA):
    parsed = feedparser.parse(feed_url)
    results = []
    limite_data = datetime.now() - timedelta(days=dias)

    for entry in parsed.entries:
        pub_date = None
        if "published" in entry:
            try:
                pub_date = parser.parse(entry.published)
                if pub_date.tzinfo is not None:
                    pub_date = pub_date.replace(tzinfo=None)
            except:
                pub_date = None

        if pub_date is None or pub_date >= limite_data:
            title = entry.get("title", "")
            summary = entry.get("summary", "")
            content = title + " " + summary
            if any(term.lower() in content.lower() for term in TERMS):
                results.append({
                    "fonte": feed_url,
                    "title": title,
                    "link": entry.get("link"),
                    "date": str(pub_date) if pub_date else None,
                    "content": content
                })
    return results

# -----------------------------
# Fun√ß√£o de scraping CMED
# -----------------------------
def buscar_cmed_precos(url):
    results = []
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        table_rows = soup.select("table tbody tr")
        for row in table_rows:
            cols = row.find_all("td")
            if len(cols) >= 3:
                medicamento = cols[0].get_text(strip=True)
                laboratorio = cols[1].get_text(strip=True)
                preco = cols[2].get_text(strip=True)
                content = f"{medicamento} {laboratorio} {preco}"
                if any(term.lower() in content.lower() for term in TERMS):
                    results.append({
                        "fonte": url,
                        "title": medicamento,
                        "link": url,
                        "date": str(datetime.now().date()),
                        "content": content
                    })
    except Exception as e:
        st.error(f"Erro ao acessar {url}: {e}")
    return results

# -----------------------------
# Bot√£o de busca
# -----------------------------
if st.button("üöÄ Buscar Not√≠cias"):
    st.info("Buscando not√≠cias... Isso pode levar alguns segundos dependendo do n√∫mero de sites.")
    all_results = []

    # RSS
    for feed in RSS_FEEDS:
        st.write(f"Buscando RSS: {feed}")
        try:
            feed_results = buscar_rss(feed)
            all_results.extend(feed_results)
        except Exception as e:
            st.error(f"Erro ao processar feed {feed}: {e}")

    # Scraping
    for url in SCRAPING_URLS:
        st.write(f"Buscando scraping: {url}")
        all_results.extend(buscar_cmed_precos(url))

    # -----------------------------
    # Mostrar resultados
    # -----------------------------
    df = pd.DataFrame(all_results).drop_duplicates(subset=["title"])

    if df.empty:
        st.warning(f"Nenhuma not√≠cia encontrada nos √∫ltimos {DIAS_BUSCA} dias para os termos pesquisados.")
    else:
        st.success(f"{len(df)} not√≠cias encontradas:")
        st.dataframe(df)

        # Exportar para Excel
        output = BytesIO()
        df.to_excel(output, index=False, engine='xlsxwriter')
        output.seek(0)

        st.download_button(
            label="üì• Baixar Excel",
            data=output,
            file_name=f"noticias_personalizadas_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
