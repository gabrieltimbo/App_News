# -*- coding: utf-8 -*-
"""Buscador de NotÃ­cias sobre Contrave / Merck.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pRzf98ffqo-D8KmamRMYR4XCiOVxjBCr
"""

# -*- coding: utf-8 -*-
"""Buscador de NotÃ­cias sobre Contrave / Merck - Streamlit"""

import streamlit as st
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from dateutil import parser
from io import BytesIO
import re

# ==============================
# ConfiguraÃ§Ãµes padrÃ£o
# ==============================
TERMS_PADRAO = [
    "Contrave", "Bupropiona", "Naltrexona",
    "Bupropiona + naltrexona", "Contrave XR",
    "Emagrecimento farmacolÃ³gico",
    "Medicamento para obesidade",
    "Tratamento de obesidade",
    "Merck",
    "Eurofarma"
]

RSS_FEEDS = [
    "https://g1.globo.com/rss/g1/economia/",
    "https://g1.globo.com/rss/g1/saude/",
    "https://www.cnnbrasil.com.br/feed/",
    "https://www.uol.com.br/feed/noticias/saude/",
    "https://www1.folha.uol.com.br/rss/saude.xml",
    "https://www.gov.br/anvisa/pt-br/assuntos/noticias?format=rss",
    "https://www.fda.gov/about-fda/press-announcements.atom",
    "https://abc-farma.com.br/feed/",
    "https://www.kairosnews.com.br/feed/",
    "https://www.pharmaceutical-technology.com/feed/",
    "https://www.merck.com/news/rss.xml",
    "https://www.eurofarma.com.br/noticias/feed/",
    "https://www.pfizer.com/news/rss.xml",
    "https://www.novartis.com/news/media-releases/rss.xml",
    "https://www.roche.com/media/newsfeed.xml"
]

SENHA_CORRETA = "minhasenha123"  # Defina sua senha

# ==============================
# Login
# ==============================
senha = st.text_input("ðŸ”‘ Digite a senha para acessar o app", type="password")
if senha != SENHA_CORRETA:
    st.warning("Senha incorreta! Acesso negado.")
    st.stop()

# ==============================
# TÃ­tulo
# ==============================
st.title("ðŸ” Buscador de NotÃ­cias sobre Contrave / Merck")
st.markdown("Busque notÃ­cias recentes em diversos sites sobre medicamentos e empresas farmacÃªuticas.")

# ==============================
# Termos de busca
# ==============================
st.subheader("ðŸ“Œ Termos de busca padrÃ£o")
st.write(", ".join(TERMS_PADRAO))

novos_termos = st.text_input("Adicionar termos extras separados por vÃ­rgula", "")
TERMS = TERMS_PADRAO + [t.strip() for t in novos_termos.split(",") if t.strip() != ""]

# ==============================
# SeleÃ§Ã£o de sites
# ==============================
st.subheader("ðŸŒ Selecionar sites para buscar")
sites_selecionados = st.multiselect(
    "Escolha os sites desejados", 
    options=RSS_FEEDS, 
    default=RSS_FEEDS  # TODOS selecionados por padrÃ£o
)

# ==============================
# Quantidade de dias
# ==============================
DIAS_BUSCA = st.number_input("Buscar notÃ­cias dos Ãºltimos X dias", min_value=1, max_value=365, value=180)

# ==============================
# Filtro exato opcional
# ==============================
filtro_exato = st.text_input("Filtrar por palavra exata (opcional)")

# ==============================
# FunÃ§Ã£o para resumir texto
# ==============================
def resumir_texto(texto, max_palavras=30):
    palavras = texto.split()
    resumo = " ".join(palavras[:max_palavras])
    if len(palavras) > max_palavras:
        resumo += "..."
    return resumo

# ==============================
# FunÃ§Ã£o para destacar palavras-chave
# ==============================
def destacar_keywords(texto, termos):
    for term in termos:
        pattern = re.compile(re.escape(term), re.IGNORECASE)
        texto = pattern.sub(f"**{term}**", texto)
    return texto

# ==============================
# FunÃ§Ã£o para buscar RSS
# ==============================
def buscar_rss(feed_url, termos, dias=180, palavra_exata=None):
    parsed = feedparser.parse(feed_url)
    results = []
    limite_data = datetime.now() - timedelta(days=dias)

    for entry in parsed.entries:
        pub_date = None
        if "published" in entry:
            try:
                pub_date = parser.parse(entry.published)
                if pub_date.tzinfo is not None:
                    pub_date = pub_date.replace(tzinfo=None)
            except:
                pub_date = None

        if pub_date is None or pub_date >= limite_data:
            title = entry.get("title", "")
            summary = entry.get("summary", "")
            content = title + " " + summary

            incluir = False
            if palavra_exata:
                incluir = palavra_exata.lower() in content.lower()
            else:
                incluir = any(term.lower() in content.lower() for term in termos)

            if incluir:
                results.append({
                    "fonte": feed_url,
                    "title": title,
                    "link": entry.get("link"),
                    "date": str(pub_date) if pub_date else None,
                    "summary": destacar_keywords(resumir_texto(content, 30), termos),
                    "content": content
                })
    return results

# ==============================
# BotÃ£o para iniciar busca
# ==============================
if st.button("ðŸš€ Buscar NotÃ­cias"):

    st.info("Buscando notÃ­cias... Isso pode levar alguns segundos dependendo do nÃºmero de sites.")

    all_results = []
    progresso = st.progress(0)
    total = len(sites_selecionados)

    for i, feed in enumerate(sites_selecionados):
        st.write(f"ðŸ”¹ Buscando: {feed}")
        try:
            feed_results = buscar_rss(feed, TERMS, dias=DIAS_BUSCA, palavra_exata=filtro_exato if filtro_exato else None)
            all_results.extend(feed_results)
        except Exception as e:
            st.error(f"Erro ao processar feed {feed}: {e}")
        progresso.progress((i + 1) / total)

    # ==============================
    # Mostrar resultados
    # ==============================
    df = pd.DataFrame(all_results).drop_duplicates(subset=["title"])

    if df.empty:
        st.warning(f"Nenhuma notÃ­cia encontrada nos Ãºltimos {DIAS_BUSCA} dias para os termos pesquisados.")
    else:
        st.success(f"{len(df)} notÃ­cias encontradas:")

        cores_sites = ["#E63946","#F1FAEE","#A8DADC","#457B9D","#1D3557","#FFBE0B","#FB5607","#8338EC","#3A86FF","#06D6A0"]
        site_cor_map = {site: cores_sites[i % len(cores_sites)] for i, site in enumerate(df['fonte'].unique())}

        with st.expander("ðŸ“‘ Mostrar tabela completa"):
            for idx, row in df.iterrows():
                st.markdown(f"<span style='color:{site_cor_map[row['fonte']]}; font-weight:bold'>{row['fonte']}</span>", unsafe_allow_html=True)
                st.markdown(f"**{row['title']}** ({row['date']})")
                st.markdown(f"{row['summary']}")
                st.markdown(f"[Link]({row['link']})")
                st.markdown("---")

        # Exportar para Excel
        output = BytesIO()
        df.to_excel(output, index=False, engine='xlsxwriter')
        output.seek(0)

        st.download_button(
            label="ðŸ“¥ Baixar Excel",
            data=output,
            file_name=f"noticias_personalizadas_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
